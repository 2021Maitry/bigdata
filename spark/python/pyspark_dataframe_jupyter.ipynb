{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DataFrame - Spark SQL using Python in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Note that the path to spark home could be different for you. This one is for HortonWorks Data Platform\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2-client\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "\n",
    "# The version for py4j could be different for you\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.4-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.10:0.4.1 pyspark-shell' \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For spark 2.3\n",
    "# import os\n",
    "# import sys\n",
    " \n",
    "# os.environ[\"SPARK_HOME\"] = \"/usr/spark2.3/\"\n",
    "# os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# # In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "# os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "# sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "# sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Entrypoint 1.x\n",
    "\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# conf = SparkConf().setAppName(\"appName\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "# specify .master(\"yarn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Connectivite Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      databaseName|\n",
      "+------------------+\n",
      "|             400bc|\n",
      "|             a_vis|\n",
      "|               abc|\n",
      "|              abc1|\n",
      "|              abc2|\n",
      "|           abhinav|\n",
      "|adarshbhoodhoo5510|\n",
      "|             adi15|\n",
      "|            aditya|\n",
      "|     aduri1ram4183|\n",
      "|        afarir4475|\n",
      "|          aishboom|\n",
      "|          ajaydb_1|\n",
      "|         ajaydb_x1|\n",
      "|   aleinsteinr2621|\n",
      "|  alokdeosingh1995|\n",
      "|alokpandey09933735|\n",
      "|   amankisku931845|\n",
      "|                an|\n",
      "|ansarirahamath7347|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\r\n",
      "<catalog>\r\n",
      "   <book id=\"bk101\">\r\n",
      "      <author>Gambardella, Matthew</author>\r\n",
      "      <title>XML Developer's Guide</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>44.95</price>\r\n",
      "      <publish_date>2000-10-01</publish_date>\r\n",
      "      <description>\r\n",
      "\r\n",
      "\r\n",
      "         An in-depth look at creating applications\r\n",
      "         with XML.This manual describes Oracle XML DB, and how you can use it to store, generate, manipulate, manage,\r\n",
      "         and query XML data in the database.\r\n",
      "\r\n",
      "\r\n",
      "         After introducing you to the heart of Oracle XML DB, namely the XMLType framework and Oracle XML DB repository,\r\n",
      "         the manual provides a brief introduction to design criteria to consider when planning your Oracle XML DB\r\n",
      "         application. It provides examples of how and where you can use Oracle XML DB.\r\n",
      "\r\n",
      "\r\n",
      "         The manual then describes ways you can store and retrieve XML data using Oracle XML DB, APIs for manipulating\r\n",
      "         XMLType data, and ways you can view, generate, transform, and search on existing XML data. The remainder of\r\n",
      "         the manual discusses how to use Oracle XML DB repository, including versioning and security,\r\n",
      "         how to access and manipulate repository resources using protocols, SQL, PL/SQL, or Java, and how to manage\r\n",
      "         your Oracle XML DB application using Oracle Enterprise Manager. It also introduces you to XML messaging and\r\n",
      "         Oracle Streams Advanced Queuing XMLType support.\r\n",
      "      </description></book><book id=\"bk102\">\r\n",
      "      <author>Ralls, Kim</author>\r\n",
      "      <title>Midnight Rain</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2000-12-16</publish_date>\r\n",
      "      <description>A former architect battles corporate zombies, \r\n",
      "      an evil sorceress, and her own childhood to become queen \r\n",
      "      of the world.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk103\">\r\n",
      "      <author>Corets, Eva</author>\r\n",
      "      <title>Maeve Ascendant</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2000-11-17</publish_date>\r\n",
      "      <description>After the collapse of a nanotechnology \r\n",
      "      society in England, the young survivors lay the \r\n",
      "      foundation for a new society.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk104\">\r\n",
      "      <author>Corets, Eva</author>\r\n",
      "      <title>Oberon's Legacy</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2001-03-10</publish_date>\r\n",
      "      <description>In post-apocalypse England, the mysterious \r\n",
      "      agent known only as Oberon helps to create a new life \r\n",
      "      for the inhabitants of London. Sequel to Maeve \r\n",
      "      Ascendant.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk105\">\r\n",
      "      <author>Corets, Eva</author>\r\n",
      "      <title>The Sundered Grail</title>\r\n",
      "      <genre>Fantasy</genre>\r\n",
      "      <price>5.95</price>\r\n",
      "      <publish_date>2001-09-10</publish_date>\r\n",
      "      <description>The two daughters of Maeve, half-sisters, \r\n",
      "      battle one another for control of England. Sequel to \r\n",
      "      Oberon's Legacy.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk106\">\r\n",
      "      <author>Randall, Cynthia</author>\r\n",
      "      <title>Lover Birds</title>\r\n",
      "      <genre>Romance</genre>\r\n",
      "      <price>4.95</price>\r\n",
      "      <publish_date>2000-09-02</publish_date>\r\n",
      "      <description>When Carla meets Paul at an ornithology \r\n",
      "      conference, tempers fly as feathers get ruffled.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk107\">\r\n",
      "      <author>Thurman, Paula</author>\r\n",
      "      <title>Splish Splash</title>\r\n",
      "      <genre>Romance</genre>\r\n",
      "      <price>4.95</price>\r\n",
      "      <publish_date>2000-11-02</publish_date>\r\n",
      "      <description>A deep sea diver finds true love twenty \r\n",
      "      thousand leagues beneath the sea.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk108\">\r\n",
      "      <author>Knorr, Stefan</author>\r\n",
      "      <title>Creepy Crawlies</title>\r\n",
      "      <genre>Horror</genre>\r\n",
      "      <price>4.95</price>\r\n",
      "      <publish_date>2000-12-06</publish_date>\r\n",
      "      <description>An anthology of horror stories about roaches,\r\n",
      "      centipedes, scorpions  and other insects.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk109\">\r\n",
      "      <author>Kress, Peter</author>\r\n",
      "      <title>Paradox Lost</title>\r\n",
      "      <genre>Science Fiction</genre>\r\n",
      "      <price>6.95</price>\r\n",
      "      <publish_date>2000-11-02</publish_date>\r\n",
      "      <description>After an inadvertant trip through a Heisenberg\r\n",
      "      Uncertainty Device, James Salway discovers the problems \r\n",
      "      of being quantum.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk110\">\r\n",
      "      <author>O'Brien, Tim</author>\r\n",
      "      <title>Microsoft .NET: The Programming Bible</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>36.95</price>\r\n",
      "      <publish_date>2000-12-09</publish_date>\r\n",
      "      <description>Microsoft's .NET initiative is explored in \r\n",
      "      detail in this deep programmer's reference.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk111\">\r\n",
      "      <author>O'Brien, Tim</author>\r\n",
      "      <title>MSXML3: A Comprehensive Guide</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>36.95</price>\r\n",
      "      <publish_date>2000-12-01</publish_date>\r\n",
      "      <description>The Microsoft MSXML3 parser is covered in \r\n",
      "      detail, with attention to XML DOM interfaces, XSLT processing, \r\n",
      "      SAX and more.</description>\r\n",
      "   </book>\r\n",
      "   <book id=\"bk112\">\r\n",
      "      <author>Galos, Mike</author>\r\n",
      "      <title>Visual Studio 7: A Comprehensive Guide</title>\r\n",
      "      <genre>Computer</genre>\r\n",
      "      <price>49.95</price>\r\n",
      "      <publish_date>2001-04-16</publish_date>\r\n",
      "      <description>Microsoft Visual Studio 7 is explored in depth,\r\n",
      "      looking at how Visual Basic, Visual C++, C#, and ASP+ are \r\n",
      "      integrated into a comprehensive development \r\n",
      "      environment.</description>\r\n",
      "   </book>\r\n",
      "</catalog>\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /data/spark/books.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"xml\").option(\"rowTag\", \"book\").load(\"/data/spark/books.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|  _id|              author|         description|          genre|price|publish_date|               title|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|bk101|Gambardella, Matthew|\n",
      "\n",
      "\n",
      "         An in...|       Computer|44.95|  2000-10-01|XML Developer's G...|\n",
      "|bk102|          Ralls, Kim|A former architec...|        Fantasy| 5.95|  2000-12-16|       Midnight Rain|\n",
      "|bk103|         Corets, Eva|After the collaps...|        Fantasy| 5.95|  2000-11-17|     Maeve Ascendant|\n",
      "|bk104|         Corets, Eva|In post-apocalyps...|        Fantasy| 5.95|  2001-03-10|     Oberon's Legacy|\n",
      "|bk105|         Corets, Eva|The two daughters...|        Fantasy| 5.95|  2001-09-10|  The Sundered Grail|\n",
      "|bk106|    Randall, Cynthia|When Carla meets ...|        Romance| 4.95|  2000-09-02|         Lover Birds|\n",
      "|bk107|      Thurman, Paula|A deep sea diver ...|        Romance| 4.95|  2000-11-02|       Splish Splash|\n",
      "|bk108|       Knorr, Stefan|An anthology of h...|         Horror| 4.95|  2000-12-06|     Creepy Crawlies|\n",
      "|bk109|        Kress, Peter|After an inadvert...|Science Fiction| 6.95|  2000-11-02|        Paradox Lost|\n",
      "|bk110|        O'Brien, Tim|Microsoft's .NET ...|       Computer|36.95|  2000-12-09|Microsoft .NET: T...|\n",
      "|bk111|        O'Brien, Tim|The Microsoft MSX...|       Computer|36.95|  2000-12-01|MSXML3: A Compreh...|\n",
      "|bk112|         Galos, Mike|Microsoft Visual ...|       Computer|49.95|  2001-04-16|Visual Studio 7: ...|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading JSON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /data/spark/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/data/spark/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "textRDD = sc.textFile(\"/data/spark/people.txt\")\n",
    "\n",
    "arrayRDD = textRDD.map(lambda x: x.split(\",\"))\n",
    "\n",
    "rowRDD = arrayRDD.map(lambda arr: Row(name=arr[0], age=int(arr[1].strip())))\n",
    "\n",
    "peopleDF = rowRDD.toDF()\n",
    "peopleDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Name: Justin']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by Spark\n",
    "teenagersDF = spark.sql(\"SELECT * FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagersDF.show()\n",
    "\n",
    "teenagersDF.rdd.map(lambda teenager: \"Name: \" + str(teenager[1])).collect()\n",
    "\n",
    "#[u'Name: Justin']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically Specifying the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Rows RDD\n",
    "filename = \"/data/spark/people.txt\"\n",
    "peopleRDD = spark.sparkContext.textFile(filename)\n",
    "peopleRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "# The schema is encoded in a string. User provided variable\n",
    "schemaString = \"name age\"\n",
    "fieldsArray = schemaString.split(\" \")\n",
    "fields = list(map(lambda f: StructField(f, StringType(), nullable = True),\n",
    "fieldsArray))\n",
    "schema = StructType(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(age,StringType,true)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrRDD = peopleRDD.map(lambda x: x.split(\",\"))\n",
    "rowRDD = arrRDD.map(lambda attr: Row(attr[0], attr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row(Michael,  29)>, <Row(Andy,  30)>, <Row(Justin,  19)>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rowRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Dataframe\n",
    "peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
